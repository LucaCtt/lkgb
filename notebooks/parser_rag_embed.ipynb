{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_debug, set_verbose\n",
    "\n",
    "set_verbose(False)\n",
    "set_debug(False)\n",
    "\n",
    "TEST_LOG_PATH = \"test_data/with_template.csv\"\n",
    "\n",
    "EMBEDDINGS_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "PARSER_MODEL = \"qwen2.5-coder:7b\"\n",
    "\n",
    "SELF_REFLECTION_STEPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the logs from the CSV file\n",
    "loader = CSVLoader(\n",
    "    file_path=TEST_LOG_PATH,\n",
    "    metadata_columns=[\"line_number\", \"tactic\", \"techniques\", \"template\"],\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# Split the logs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Load the embeddings model\n",
    "local_embeddings = OllamaEmbeddings(model=EMBEDDINGS_MODEL)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=local_embeddings,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# Create the parser model\n",
    "parser_model = ChatOllama(\n",
    "    model=PARSER_MODEL,\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_logs_for_prompt(log: str, similar_logs: list[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Formats the given log and a list of similar logs into a string suitable for use in a prompt.\n",
    "\n",
    "    This function takes an input log and a list of similar logs, extracts the relevant content from each log,\n",
    "    and formats them into a single string that can be used as input for a prompt. The logs are enclosed in\n",
    "    double quotes and separated by commas, and the entire list is enclosed in square brackets.\n",
    "\n",
    "    Args:\n",
    "        log (str): The input log to be formatted.\n",
    "        similar_logs (list[Document]): A list of Document objects representing similar logs.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the input log and similar logs, suitable for use in a prompt.\n",
    "\n",
    "    \"\"\"\n",
    "    # Cut the \"text: \" prefix from the page content of each log\n",
    "    all_logs = [log, *[similar_log.page_content[len(\"text: \") :] for similar_log in similar_logs]]\n",
    "    all_logs = [f'\"{log}\"' for log in all_logs]\n",
    "\n",
    "    return \"[\" + \", \".join(all_logs) + \"]\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You will be provided with a list of logs. You must identify and abstract all the dynamic variables in logs with '<*>' and output ONE static log template that matches all the logs. Datetimes and ip addresses should each be abstracted as a standalone '<*>'.  Print the input logs' template delimited by backticks.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            'Log list: [\"2022-01-21 00:09:11 try to connect to host: 172.16.254.1:5000, finished.\", \"2022-01-21 00:09:11 try to connect to host: 173.16.254.2:6060, finished.\"]',\n",
    "        ),\n",
    "        (\"ai\", \"<*> try to connect to host: <*>, finished.\"),\n",
    "        (\"human\", \"Log list: {logs}\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(logs=lambda inputs: format_logs_for_prompt(inputs[\"input_log\"], inputs[\"similar_logs\"]))\n",
    "    | prompt\n",
    "    | parser_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "\n",
    "def get_template(log: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a log, this function identifies and returns a template that matches the log.\n",
    "    It first searches for very similar logs in the vector store and checks if their templates match the current log.\n",
    "    If no matching template is found, it searches for sufficiently similar logs and uses them to generate a template.\n",
    "\n",
    "    Args:\n",
    "        log (str): The log for which the template needs to be identified.\n",
    "\n",
    "    Returns:\n",
    "        str: The identified template for the given log.\n",
    "\n",
    "    \"\"\"\n",
    "    similarity_question = f'Which logs are most similar to \"{log}\"?'\n",
    "\n",
    "    # Check if there are very similar logs\n",
    "    # Assumption: the returned documents are sorted by most relevant first\n",
    "    very_similar_logs = vectorstore.similarity_search_with_relevance_scores(\n",
    "        similarity_question,\n",
    "        score_threshold=0.7,\n",
    "        k=10,\n",
    "        filter={\"template\": {\"$ne\": \"\"}},\n",
    "    )\n",
    "\n",
    "    # If there are very similar logs,\n",
    "    # check if their template matches with the current log\n",
    "    if len(very_similar_logs) > 0:\n",
    "        for similar_log in very_similar_logs:\n",
    "            if re.match(similar_log[0].metadata[\"template\"], log):\n",
    "                return similar_log[0].metadata[\"template\"]\n",
    "\n",
    "    # If there are no very similar logs or their template doesn't match,\n",
    "    # find sufficiently similar logs\n",
    "    similar_logs = vectorstore.similarity_search_with_relevance_scores(similarity_question, k=5, score_threshold=0.5)\n",
    "    similar_logs = [log[0] for log in similar_logs]\n",
    "\n",
    "    # Perform self-reflection to verify that the template\n",
    "    # matches both the current and similar logs\n",
    "    self_reflection_countdown = SELF_REFLECTION_STEPS\n",
    "\n",
    "    while self_reflection_countdown > 0:\n",
    "        self_reflection_countdown -= 1\n",
    "\n",
    "        # Find the template using the current log and the similar logs\n",
    "        template = chain.invoke({\"input_log\": log, \"similar_logs\": similar_logs})\n",
    "\n",
    "        # Replace all of the <*> in the template with (.*?)\n",
    "        template = template.replace(\"<*>\", \"(.*?)\")\n",
    "\n",
    "        # Check that the current log matches the template\n",
    "        if not re.match(template, log):\n",
    "            continue\n",
    "\n",
    "        # Check that all the similar logs match the template\n",
    "        for similar_log in similar_logs:\n",
    "            if not re.match(template, similar_log.page_content):\n",
    "                continue\n",
    "\n",
    "        # If the template matches all the logs, stop the self-reflection loop\n",
    "        break\n",
    "\n",
    "    # Update the template metadata value for the similar logs\n",
    "    for similar_log in similar_logs:\n",
    "        similar_log.metadata[\"template\"] = template\n",
    "        vectorstore.update_document(document_id=similar_log.id, document=similar_log)\n",
    "\n",
    "    # Save the new logs to the vector store\n",
    "    vectorstore.add_documents([Document(id=uuid.uuid4(), page_content=log, metadata={\"template\": template})])\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(.*?) (.*?)/(.*?) peer info: (.*?)=(.*?)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_template(\"2022-01-21 01:04:19 jhall/192.168.230.165:46011 peer info: IV_TCPNL=1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
